---
title: "Ranking traits by their importance to growth rate"
author: "Jody Daniel"
date: "`r Sys.Date()`"
test: " `r getRversion()`"
output:
 markdowntemplates::skeleton:
    highlight: tango
    toc: TRUE
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE,  out.extra = " ")
```

## Background

We plan to build a general linear model that predicts tree growth rate. But, we have lots of traits as predictors - environmental and physical. Based on Julie's work, we know that the physical traits are poorly summarized assuming a linear, orthogonal relationship. For the environmental traits, her work suggests that a PCA does a good job in reducing features. As such, in jupyter notebook, we created a new data set that includes these environmental traits as PCs, naming each axis based on the dominant vector relationships. 

There was alot of missing data, so we used predictive mean matching to fill in gaps. Based on preliminary analyses, there isn't much of a difference between the imputed and raw data. So, we can proceed with the imputed data for these analyses.

Before building the GLM, we need to pre-determine which of the traits are most important in predicting growth rate. It is not advisable that we throw a bunch of predictors at the GLM and pull out the important ones. Its best we use another modelling framework for feature reduction and use the results to inform which predictors will go into the GLM. In the past analyses, I used xgboost - here I will use random forest in tidy models. 

1. Prepare data for modelling using recipe - training and test  
2. Build random forest model
3. Assess performance and determine which features are most important
4. Compare performance across the 3 growth rate measures

``` {r message = FALSE}
library(knitr)
library(kableExtra)
library(tidyverse)
library(dplyr)
library(here)
library(skimr)
library(reshape2)
library(tidymodels)
library(qdapTools)
library(rsample)
library(corrr)
library(broom)
library(vegan)
library(extrafont)
library(viridis)
library(car)
source(here("scripts/archive/1. functions.R"))
theme_set(theme_special())
```

### Preparing data for random forest model

Assuming that the first 10000 rows describes the data well, I can use col_types = cols(). If not, I will need to specify the class of each column.
```{r message = FALSE}
# prior error suggests that there are single quotation marks used to parse numbers
# I specify that we should read those as a thousand separator
rgr_msh <- read_csv(here("data/RGR_MSH_PCA.csv"),
                        guess_max = 10000,
                        col_types = cols())

rgr_msh_na <- read_csv(here("data/RGR_MSH_PCA_NA.csv"),
                        guess_max = 10000,
                        col_types = cols())

# there are NAs in the predictors - let's drop those
rgr_msh <- rgr_msh[-(which(is.na(rgr_msh$BAI_GR))),]
rgr_msh$BIO_GR <- as.numeric(rgr_msh$BIO_GR)
rgr_msh_na <- rgr_msh_na[-(which(is.na(rgr_msh_na$BAI_GR))),]
# now, let's remove columns that are either too correlated are would not be useful
labels_rgr_msh <- read_csv(here("data/labels.csv"),
                        guess_max = 10000,
                        col_types = cols())

skim(rgr_msh)
skim(rgr_msh_na)
```



``` {r message = FALSE}

# need a training and test set assess the performance of the model
# a 70:30 split is typical
# first, I will work with the imputed data
set.seed(634)
split_train_test <-
  initial_split(
    data = rgr_msh, 
    prop = 0.70) 
train_rgr_msh <- split_train_test %>% training() 
test_dta_msh  <- split_train_test %>% testing()

# now, I will work with the raw data
set.seed(634)
split_train_test_na <-
  initial_split(
    data = rgr_msh_na, 
    prop = 0.70) 
train_rgr_msh_na <- split_train_test_na %>% training() 
test_dta_msh_na  <- split_train_test_na %>% testing()

```

## Building the random forest modelS

The first step in building the random forest models is to tuning. There are four models to tune, because we have two datasets and two predictors. The predictors are basal area growth rate and biomass growth rate. Also, we have a raw dataset, and another that was built from predictive mean matching - the past examinations suggest they are about the same, but I'd like to ensure that the converge on model predictions. 

``` {r echo = FALSE}
# first, we must tune the random forest model. i will use a 80:20 split.
# i will tune for the number of predictors that will be randomly pooled/available
# for splitting at each node (mtry)
# i will also tune min_n, which is minimum number of observation required to split a node further

# because there are two predictors with 2 different versions (imputed vs raw) 
# I will need to validate/build 4 models

cores <- parallel::detectCores()
randomforest_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("regression")

# need split the training data to tune the model (validation)
set.seed(345)
val_set_bai <- validation_split(train_rgr_msh[,-(which(names(train_rgr_msh)=="BIO_GR"))],  
                            prop = 0.80)
val_set_bio <- validation_split(train_rgr_msh[,-(which(names(train_rgr_msh)=="BAI_GR"))],  
                            prop = 0.80)

#### making a recipe for what goes into each of the four models
# imputed data
randomforest_recipe_bai <- 
  recipe(BAI_GR ~ ., data = train_rgr_msh[,-(which(names(train_rgr_msh)=="BIO_GR"))])
randomforest_recipe_bio <- 
  recipe(BIO_GR ~ ., data = train_rgr_msh[,-(which(names(train_rgr_msh)=="BAI_GR"))])


# the workflow helps in given steps in the tuning process
# imputed data
randomforest_workflow_bai <- 
  workflow() %>% 
  add_model(randomforest_mod) %>% 
  add_recipe(randomforest_recipe_bai)
randomforest_workflow_bio <- 
  workflow() %>% 
  add_model(randomforest_mod) %>% 
  add_recipe(randomforest_recipe_bio)


# now, we can set the terms for what we use to validate
# imputed data
randomforest_res_bai <- 
  randomforest_workflow_bai %>% 
  tune_grid(val_set_bai,
            grid = 10,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
randomforest_res_bio <- 
  randomforest_workflow_bio %>% 
  tune_grid(val_set_bio,
            grid = 10,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))

# now, we can look at the model performance
# Basal Area Growth Rate
randomforest_res_bai %>% 
  show_best(metric = "rmse")


# Biomass Growth Rate
randomforest_res_bio %>% 
  show_best(metric = "rmse")

```


```{r echo=FALSE}
library(vip)
random_forest_regressor_bai <-
  rand_forest(mtry = 28, min_n = 4, trees = 1000) %>% 
  set_engine("ranger", num.threads = cores, importance = "impurity") %>% 
  set_mode("regression") %>% 
  fit(BAI_GR ~ ., data = train_rgr_msh[,-(which(names(train_rgr_msh)=="BIO_GR"))])
random_forest_regressor_bio <-
  rand_forest(mtry = 32, min_n = 38, trees = 1000) %>% 
  set_engine("ranger", num.threads = cores, importance = "impurity") %>% 
  set_mode("regression") %>% 
  fit(BIO_GR ~ ., data = train_rgr_msh[,-(which(names(train_rgr_msh)=="BAI_GR"))])

# is the relative importance of predictors the same as with the logistic regression?
random_forest_regressor_bai%>%
  vip(geom = "point")

random_forest_regressor_bio%>%
  vip(geom = "point")
```
